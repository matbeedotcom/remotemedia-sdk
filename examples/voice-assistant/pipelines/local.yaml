# Local Mode Pipeline
# All processing happens on the user's device

name: voice-assistant-local
version: "1.0"
description: Voice assistant running entirely locally

nodes:
  - id: mic_input
    node_type: AudioInput
    config:
      sample_rate: 16000
      channels: 1
      format: f32
      source: microphone

  - id: vad
    node_type: SileroVAD
    config:
      threshold: 0.5
      min_speech_duration_ms: 250
      min_silence_duration_ms: 500
      pre_speech_pad_ms: 300

  - id: whisper
    node_type: WhisperSTT
    executor: multiprocess
    config:
      model: base.en
      language: en
      task: transcribe

  - id: llm
    node_type: OllamaLLM
    executor: multiprocess
    config:
      model: llama3.2:1b
      system_prompt: |
        You are a helpful voice assistant. Keep responses concise
        and conversational since they will be spoken aloud.
      max_tokens: 150
      temperature: 0.7

  - id: tts
    node_type: KokoroTTS
    executor: multiprocess
    config:
      voice: af_bella
      speed: 1.0
      sample_rate: 24000

  - id: resample
    node_type: AudioResample
    config:
      target_sample_rate: 48000
      quality: high

  - id: speaker_output
    node_type: AudioOutput
    config:
      sample_rate: 48000
      channels: 1
      format: f32
      destination: speaker

connections:
  - from: mic_input
    to: vad
  - from: vad
    to: whisper
  - from: whisper
    to: llm
  - from: llm
    to: tts
  - from: tts
    to: resample
  - from: resample
    to: speaker_output
