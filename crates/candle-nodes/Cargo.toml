[package]
name = "remotemedia-candle-nodes"
version.workspace = true
edition.workspace = true
rust-version.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true
description = "Candle ML inference nodes for RemoteMedia pipelines"

[dependencies]
# Core workspace dependencies
tokio.workspace = true
serde.workspace = true
serde_json.workspace = true
async-trait.workspace = true
tracing.workspace = true
thiserror.workspace = true
anyhow.workspace = true

# Candle ML framework (git main for CUDA 13 support via latest cudarc)
candle-core = { git = "https://github.com/huggingface/candle", optional = true }
candle-nn = { git = "https://github.com/huggingface/candle", optional = true }
candle-transformers = { git = "https://github.com/huggingface/candle", optional = true }
candle-onnx = { git = "https://github.com/huggingface/candle", optional = true }

# HuggingFace Hub for model downloads
hf-hub = { version = "0.4", default-features = false, features = ["tokio", "ureq", "rustls-tls"], optional = true }

# Tokenizers for LLM text processing
tokenizers = { version = "0.20", optional = true }

# Audio processing for Whisper
rubato.workspace = true

# RemoteMedia core for StreamingNode trait
# Use default-features = false to avoid pulling in video/FFmpeg dependencies
remotemedia-core = { path = "../core", default-features = false }

# Utilities
dirs.workspace = true
rand.workspace = true

[dev-dependencies]
tokio-test.workspace = true
criterion.workspace = true
tempfile.workspace = true
hound.workspace = true
tracing-subscriber.workspace = true

[features]
default = []

# Model family features
whisper = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:hf-hub", "dep:tokenizers"]
yolo = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:hf-hub"]
llm = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:hf-hub", "dep:tokenizers"]
vad = ["dep:candle-core", "dep:candle-onnx", "dep:hf-hub"]

# Hardware acceleration features
cuda = ["candle-core?/cuda", "candle-nn?/cuda", "candle-transformers?/cuda"]
metal = ["candle-core?/metal", "candle-nn?/metal", "candle-transformers?/metal"]

# Convenience feature for all models
all-models = ["whisper", "yolo", "llm", "vad"]

[[bench]]
name = "inference_bench"
harness = false

[[example]]
name = "whisper_test"
required-features = ["whisper"]

[[example]]
name = "vad_test"
required-features = ["vad"]
