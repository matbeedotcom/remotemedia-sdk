{
  "version": "v1",
  "metadata": {
    "name": "GPU_Inference_Pipeline",
    "description": "ML inference pipeline requiring GPU"
  },
  "nodes": [
    {
      "id": "video-input",
      "node_type": "VideoInput",
      "params": {
        "resolution": "1920x1080"
      }
    },
    {
      "id": "llm-inference",
      "node_type": "LLMInferenceNode",
      "params": {
        "model": "llama-70b-large",
        "device": "cuda"
      },
      "capabilities": {
        "gpu": {
          "type": "cuda",
          "min_memory_gb": 24.0,
          "required": true
        },
        "memory_gb": 32.0
      }
    },
    {
      "id": "output",
      "node_type": "TextOutput",
      "params": {}
    }
  ],
  "connections": [
    {
      "from": "video-input",
      "to": "llm-inference"
    },
    {
      "from": "llm-inference",
      "to": "output"
    }
  ]
}
