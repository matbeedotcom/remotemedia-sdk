//! Bidirectional streaming RPC handler for StreamPipeline
//!
//! This module implements the StreamingPipelineService trait for real-time
//! chunk-by-chunk audio processing with <50ms latency per chunk.
//!
//! # Architecture
//!
//! - **StreamingServiceImpl**: Main service implementation with session management
//! - **StreamSession**: Per-session state (manifest, executor, metrics, sequence tracking)
//! - **stream_pipeline**: Bidirectional stream handler loop
//!
//! # Flow
//!
//! 1. Client sends StreamInit with manifest ‚Üí Server responds with StreamReady
//! 2. Client sends AudioChunk messages ‚Üí Server processes and returns ChunkResult
//! 3. Periodic StreamMetrics sent every 10 chunks
//! 4. Client sends StreamControl::CLOSE ‚Üí Server flushes and sends StreamClosed
//!
//! # Performance
//!
//! - Target: <50ms average latency per chunk (User Story 3)
//! - Bounded buffer to prevent memory bloat
//! - Backpressure via STREAM_ERROR_BUFFER_OVERFLOW

// Internal infrastructure - some fields/methods for future use
#![allow(dead_code)]

use crate::generated::{
    stream_control::Command, stream_request::Request as StreamRequestType,
    stream_response::Response as StreamResponseType, AudioBuffer as ProtoAudioBuffer, AudioChunk,
    ChunkResult, ErrorResponse, ErrorType, ExecutionMetrics, StreamClosed, StreamControl,
    StreamInit, StreamMetrics, StreamReady, StreamRequest, StreamResponse,
};
use crate::metrics::ServiceMetrics;
use crate::session_router::{DataPacket, SessionRouter};
use crate::ServiceError;
use remotemedia_runtime_core::{
    audio::AudioBuffer as RuntimeAudioBuffer,
    data::RuntimeData,
    manifest::Manifest,
    nodes::{python_streaming::PythonStreamingNode, StreamingNode, StreamingNodeRegistry},
    transport::PipelineRunner,
};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::{Mutex, RwLock};
use tokio::task::JoinHandle;
use tonic::{Request, Response, Status, Streaming};
use tracing::{debug, error, info, warn};
use uuid::Uuid;

/// Maximum number of chunks buffered before backpressure
const MAX_BUFFER_CHUNKS: usize = 10;

/// Maximum session idle time before timeout (seconds)
const SESSION_TIMEOUT_SECS: u64 = 300; // 5 minutes

/// Global node cache TTL (seconds) - how long to keep cached nodes after last use
const GLOBAL_NODE_CACHE_TTL_SECS: u64 = 600; // 10 minutes

/// Interval for cache cleanup checks (seconds)
const CACHE_CLEANUP_INTERVAL_SECS: u64 = 60; // 1 minute

/// Frequency of metrics updates (every N chunks)
const METRICS_UPDATE_INTERVAL: u64 = 10;

/// Node cache entry with timestamp for TTL management
struct CachedNode {
    node: Arc<Box<dyn StreamingNode>>,
    /// For Python streaming nodes, store the unwrapped instance to access process_streaming()
    py_streaming_node: Option<Arc<PythonStreamingNode>>,
    last_used: Instant,
}

/// Streaming pipeline service implementation
pub struct StreamingServiceImpl {
    /// Active streaming sessions (keyed by session_id)
    sessions: Arc<RwLock<HashMap<String, Arc<Mutex<StreamSession>>>>>,

    /// Authentication configuration
    auth_config: crate::auth::AuthConfig,

    /// Resource limits
    limits: crate::limits::ResourceLimits,

    /// Prometheus metrics
    metrics: Arc<ServiceMetrics>,

    /// Pipeline runner (encapsulates executor and registries)
    runner: Arc<PipelineRunner>,

    /// Global node cache (shared across all sessions)
    /// Key: "{node_type}:{json_params_hash}", Value: cached node with timestamp
    global_node_cache: Arc<RwLock<HashMap<String, CachedNode>>>,
}

impl StreamingServiceImpl {
    /// Create new streaming service instance
    pub fn new(
        auth_config: crate::auth::AuthConfig,
        limits: crate::limits::ResourceLimits,
        metrics: Arc<ServiceMetrics>,
        runner: Arc<PipelineRunner>,
    ) -> Self {
        let global_node_cache: Arc<RwLock<HashMap<String, CachedNode>>> =
            Arc::new(RwLock::new(HashMap::new()));

        // Spawn background task to periodically clean up expired cache entries
        let cache_for_cleanup = global_node_cache.clone();
        tokio::spawn(async move {
            let mut interval =
                tokio::time::interval(std::time::Duration::from_secs(CACHE_CLEANUP_INTERVAL_SECS));

            loop {
                interval.tick().await;

                // Clean up expired cache entries
                let mut cache = cache_for_cleanup.write().await;
                let before_count = cache.len();

                cache.retain(|key, cached_node| {
                    let age_secs = cached_node.last_used.elapsed().as_secs();
                    let keep = age_secs < GLOBAL_NODE_CACHE_TTL_SECS;
                    if !keep {
                        info!("üóëÔ∏è Expired cached node '{}' (idle for {}s)", key, age_secs);
                    }
                    keep
                });

                let removed_count = before_count - cache.len();
                if removed_count > 0 {
                    info!(
                        "üßπ Cache cleanup: removed {} expired nodes ({} remaining)",
                        removed_count,
                        cache.len()
                    );
                }
            }
        });

        Self {
            sessions: Arc::new(RwLock::new(HashMap::new())),
            auth_config,
            limits,
            metrics,
            runner,
            global_node_cache,
        }
    }

    /// Get number of active sessions
    pub async fn active_session_count(&self) -> usize {
        self.sessions.read().await.len()
    }
}

/// Per-session state for streaming execution
pub(crate) struct StreamSession {
    /// Unique session identifier
    pub(crate) session_id: String,

    /// Parsed pipeline manifest
    pub(crate) manifest: Manifest,

    /// Expected next sequence number
    next_sequence: u64,

    /// Total chunks processed
    chunks_processed: u64,

    /// Total items processed (samples, frames, tokens, etc.)
    total_items: u64,

    /// Data type distribution
    data_type_counts: HashMap<String, u64>,

    /// Total chunks dropped (backpressure)
    chunks_dropped: u64,

    /// Cumulative processing time (milliseconds)
    cumulative_processing_time_ms: f64,

    /// Peak memory usage (bytes)
    peak_memory_bytes: u64,

    /// Current buffer occupancy (items)
    buffer_items: u64,

    /// Session creation time
    created_at: Instant,

    /// Last activity time (for timeout detection)
    last_activity: Instant,

    /// Recommended chunk size (samples)
    recommended_chunk_size: u64,

    /// Node cache: reuses initialized nodes across chunks
    /// Key: node_id, Value: cached StreamingNode instance
    /// This prevents expensive re-initialization (e.g., ML model loading)
    pub(crate) node_cache: HashMap<String, Arc<Box<dyn StreamingNode>>>,

    /// Cache hits for this session (Feature 005)
    pub(crate) cache_hits: u64,

    /// Cache misses for this session (Feature 005)
    pub(crate) cache_misses: u64,

    /// Input sender to feed chunks to the session router
    pub(crate) router_input: Option<tokio::sync::mpsc::UnboundedSender<DataPacket>>,

    /// Router task handle
    pub(crate) router_task: Option<JoinHandle<()>>,

    /// Router shutdown signal sender
    pub(crate) router_shutdown: Option<tokio::sync::mpsc::Sender<()>>,
}

impl StreamSession {
    /// Create new session from StreamInit request
    fn new(session_id: String, manifest: Manifest, recommended_chunk_size: u64) -> Self {
        let now = Instant::now();
        Self {
            session_id,
            manifest,
            next_sequence: 0,
            chunks_processed: 0,
            total_items: 0,
            data_type_counts: HashMap::new(),
            chunks_dropped: 0,
            cumulative_processing_time_ms: 0.0,
            peak_memory_bytes: 0,
            buffer_items: 0,
            created_at: now,
            last_activity: now,
            recommended_chunk_size,
            node_cache: HashMap::new(),
            cache_hits: 0,
            cache_misses: 0,
            router_input: None,
            router_task: None,
            router_shutdown: None,
        }
    }

    /// Update last activity timestamp
    fn touch(&mut self) {
        self.last_activity = Instant::now();
    }

    /// Check if session has timed out
    fn is_timed_out(&self) -> bool {
        self.last_activity.elapsed().as_secs() > SESSION_TIMEOUT_SECS
    }

    /// Get number of cached nodes
    fn cached_nodes_count(&self) -> usize {
        self.node_cache.len()
    }

    /// Clear node cache (called on session cleanup)
    fn clear_node_cache(&mut self) {
        let count = self.node_cache.len();
        self.node_cache.clear();
        if count > 0 {
            info!(
                "üóëÔ∏è Cleared {} cached nodes for session {}",
                count, self.session_id
            );
        }
    }

    /// Shutdown the router and all node processing
    async fn shutdown_router(&mut self) {
        info!(
            "[ROUTER-SHUTDOWN] Starting shutdown for session '{}'",
            self.session_id
        );

        // Drop the input sender to close the router's input channel
        info!("[ROUTER-SHUTDOWN] Dropping router input channel...");
        self.router_input.take();

        // Send shutdown signal to router
        if let Some(shutdown_tx) = self.router_shutdown.take() {
            info!("[ROUTER-SHUTDOWN] Sending shutdown signal to router...");
            let _ = shutdown_tx.send(()).await;
            info!("[ROUTER-SHUTDOWN] Shutdown signal sent");
        } else {
            info!("[ROUTER-SHUTDOWN] No shutdown channel available");
        }

        // Wait for router task to complete
        if let Some(task) = self.router_task.take() {
            info!("[ROUTER-SHUTDOWN] Waiting for router task to complete...");
            match tokio::time::timeout(std::time::Duration::from_millis(500), task).await {
                Ok(Ok(_)) => info!(
                    "[ROUTER-SHUTDOWN] ‚úÖ Router task completed for session '{}'",
                    self.session_id
                ),
                Ok(Err(e)) => error!(
                    "[ROUTER-SHUTDOWN] Router task failed for session '{}': {}",
                    self.session_id, e
                ),
                Err(_) => warn!(
                    "[ROUTER-SHUTDOWN] ‚è±Ô∏è Router task timeout for session '{}', continuing anyway",
                    self.session_id
                ),
            }
        } else {
            info!("[ROUTER-SHUTDOWN] No router task to wait for");
        }

        info!(
            "[ROUTER-SHUTDOWN] Shutdown complete for session '{}'",
            self.session_id
        );
    }

    /// Validate sequence number (detect gaps or out-of-order)
    fn validate_sequence(&mut self, sequence: u64) -> Result<(), ServiceError> {
        if sequence < self.next_sequence {
            return Err(ServiceError::Validation(format!(
                "Out-of-order chunk: expected sequence {}, got {}",
                self.next_sequence, sequence
            )));
        }

        if sequence > self.next_sequence {
            let gap = sequence - self.next_sequence;
            warn!(
                session_id = %self.session_id,
                expected = self.next_sequence,
                received = sequence,
                gap = gap,
                "Missing chunks detected"
            );
            // For now, accept the chunk but log the gap
            // In production, might want to return STREAM_ERROR_INVALID_SEQUENCE
        }

        self.next_sequence = sequence + 1;
        Ok(())
    }

    /// Record processing metrics for a chunk
    fn record_chunk_metrics(
        &mut self,
        processing_time_ms: f64,
        items: u64,
        memory_bytes: u64,
        data_type: &str,
    ) {
        self.chunks_processed += 1;
        self.total_items += items;
        self.cumulative_processing_time_ms += processing_time_ms;

        // Update data type breakdown
        *self
            .data_type_counts
            .entry(data_type.to_string())
            .or_insert(0) += 1;

        if memory_bytes > self.peak_memory_bytes {
            self.peak_memory_bytes = memory_bytes;
        }

        self.touch();
    }

    /// Calculate average latency across all processed chunks
    fn average_latency_ms(&self) -> f64 {
        if self.chunks_processed == 0 {
            0.0
        } else {
            self.cumulative_processing_time_ms / self.chunks_processed as f64
        }
    }

    /// Generate StreamMetrics message
    fn create_metrics(&self, cached_nodes_count: u64) -> StreamMetrics {
        let cache_total = self.cache_hits + self.cache_misses;
        let cache_hit_rate = if cache_total > 0 {
            self.cache_hits as f64 / cache_total as f64
        } else {
            0.0
        };

        StreamMetrics {
            session_id: self.session_id.clone(),
            chunks_processed: self.chunks_processed,
            average_latency_ms: self.average_latency_ms(),
            total_items: self.total_items,
            buffer_items: self.buffer_items,
            chunks_dropped: self.chunks_dropped,
            peak_memory_bytes: self.peak_memory_bytes,
            data_type_breakdown: self.data_type_counts.clone(),
            cache_hits: self.cache_hits,
            cache_misses: self.cache_misses,
            cached_nodes_count,
            cache_hit_rate,
        }
    }

    /// Generate final ExecutionMetrics for StreamClosed
    fn create_final_metrics(&self) -> ExecutionMetrics {
        ExecutionMetrics {
            wall_time_ms: self.created_at.elapsed().as_secs_f64() * 1000.0,
            cpu_time_ms: self.cumulative_processing_time_ms, // Approximate
            memory_used_bytes: self.peak_memory_bytes,
            node_metrics: HashMap::new(), // TODO: Populate from executor
            serialization_time_ms: 0.0,   // Not tracked for streaming
            proto_to_runtime_ms: 0.0,     // Not tracked yet
            runtime_to_proto_ms: 0.0,     // Not tracked yet
            data_type_breakdown: self.data_type_counts.clone(),
        }
    }
}

#[tonic::async_trait]
impl crate::StreamingPipelineService for StreamingServiceImpl {
    type StreamPipelineStream =
        tokio_stream::wrappers::ReceiverStream<Result<StreamResponse, Status>>;

    async fn stream_pipeline(
        &self,
        request: Request<Streaming<StreamRequest>>,
    ) -> Result<Response<Self::StreamPipelineStream>, Status> {
        info!("StreamPipeline RPC invoked");

        // Preview feature header validation (from initial request metadata)
        // Note: Feature flag validation removed - configure via ServiceConfig if needed
        if let Some(_hdr_val) = request.metadata().get("x-preview-features") {
            // Preview features can be enabled/disabled via ServiceConfig
            // For now, we allow all preview features
            info!("Preview features requested (validation skipped)");
        }

        let (tx, rx) = tokio::sync::mpsc::channel(32);
        let mut stream = request.into_inner();
        let sessions = self.sessions.clone();
        let metrics = self.metrics.clone();
        #[cfg(feature = "multiprocess")]
        let multiprocess_executor = self.multiprocess_executor.clone();

        // Get executor and registry from PipelineRunner
        let executor = self.runner.executor();
        let streaming_registry = self.runner.create_streaming_registry();
        let global_node_cache = self.global_node_cache.clone();

        // Spawn async task to handle bidirectional streaming
        tokio::spawn(async move {
            #[cfg(feature = "multiprocess")]
            let result = handle_stream(
                &mut stream,
                tx.clone(),
                sessions,
                executor,
                metrics,
                streaming_registry,
                global_node_cache,
                multiprocess_executor,
            )
            .await;

            #[cfg(not(feature = "multiprocess"))]
            let result = handle_stream(
                &mut stream,
                tx.clone(),
                sessions,
                executor,
                metrics,
                streaming_registry,
                global_node_cache,
            )
            .await;

            if let Err(e) = result {
                error!(error = %e, "Stream handling error");
                let error_response = ErrorResponse {
                    error_type: ErrorType::Internal as i32,
                    message: e.to_string(),
                    failing_node_id: String::new(),
                    context: String::new(),
                    stack_trace: String::new(),
                };
                let response = StreamResponse {
                    response: Some(StreamResponseType::Error(error_response)),
                };
                let _ = tx.send(Ok(response)).await;
            }
        });

        Ok(Response::new(tokio_stream::wrappers::ReceiverStream::new(
            rx,
        )))
    }
}

/// Handle bidirectional stream (runs in async task)
async fn handle_stream(
    stream: &mut Streaming<StreamRequest>,
    tx: tokio::sync::mpsc::Sender<Result<StreamResponse, Status>>,
    sessions: Arc<RwLock<HashMap<String, Arc<Mutex<StreamSession>>>>>,
    executor: Arc<remotemedia_runtime_core::executor::Executor>,
    metrics: Arc<ServiceMetrics>,
    streaming_registry: Arc<StreamingNodeRegistry>,
    global_node_cache: Arc<RwLock<HashMap<String, CachedNode>>>,
    #[cfg(feature = "multiprocess")] multiprocess_executor: Option<
        Arc<crate::python::multiprocess::MultiprocessExecutor>,
    >,
) -> Result<(), ServiceError> {
    let mut session: Option<Arc<Mutex<StreamSession>>> = None;
    let mut session_id = String::new();

    // Main stream loop
    while let Some(request_result) = stream
        .message()
        .await
        .map_err(|e| ServiceError::Internal(format!("Stream receive error: {}", e)))?
    {
        match request_result.request {
            Some(StreamRequestType::Init(init)) => {
                // Handle StreamInit (must be first message)
                if session.is_some() {
                    return Err(ServiceError::Validation(
                        "StreamInit already received".to_string(),
                    ));
                }

                debug!("Processing StreamInit");
                let (new_session_id, ready) =
                    handle_stream_init(init, &sessions, executor.clone()).await?;
                session_id = new_session_id.clone();
                session = Some(sessions.read().await.get(&session_id).unwrap().clone());

                // Create and start the SessionRouter for this session
                let sess = session.as_ref().unwrap();

                // Create the session router with graph validation (spec 021)
                // This validates the pipeline graph (cycles, missing nodes) before streaming starts
                let (mut router, shutdown_tx) = SessionRouter::new(
                    session_id.clone(),
                    streaming_registry.clone(),
                    sess.clone(),
                    tx.clone(),
                )
                .await
                .map_err(|e| {
                    error!("Failed to create session router: {}", e);
                    ServiceError::Validation(format!("Pipeline graph validation failed: {}", e))
                })?;

                // Set multiprocess executor if available
                #[cfg(feature = "multiprocess")]
                if let Some(ref mp_executor) = multiprocess_executor {
                    router.set_multiprocess_executor(mp_executor.clone());
                }

                // üî• Pre-initialize all nodes before streaming starts
                // CRITICAL: Do this WITHOUT holding the session lock to avoid deadlock
                // (get_or_create_node needs to acquire the lock)
                info!("üî• Pre-initializing nodes for session '{}'", session_id);
                router.pre_initialize_all_nodes().await.map_err(|e| {
                    error!("Failed to pre-initialize nodes: {}", e);
                    ServiceError::Internal(format!("Node pre-initialization failed: {}", e))
                })?;
                info!("‚úÖ All nodes ready, starting router");

                // Get the input sender before starting
                let input_sender = router.get_input_sender();

                // Start the router task
                let task = router.start();

                // Now acquire lock to store router state
                {
                    let mut sess_guard = sess.lock().await;
                    sess_guard.router_task = Some(task);

                    // Store the input sender for feeding chunks
                    sess_guard.router_input = Some(input_sender);

                    // Store the shutdown sender for cleanup
                    sess_guard.router_shutdown = Some(shutdown_tx);

                    info!("üöÄ SessionRouter started for session '{}'", session_id);
                }

                // Record metrics
                metrics.record_stream_start();

                // Send StreamReady response
                let response = StreamResponse {
                    response: Some(StreamResponseType::Ready(ready)),
                };
                tx.send(Ok(response)).await.map_err(|_| {
                    ServiceError::Internal("Failed to send StreamReady".to_string())
                })?;
            }

            Some(StreamRequestType::AudioChunk(chunk)) => {
                // Handle AudioChunk
                let sess = session.as_ref().ok_or_else(|| {
                    ServiceError::Validation("StreamInit required before AudioChunk".to_string())
                })?;

                let chunk_start = Instant::now();
                debug!(sequence = chunk.sequence, "Processing AudioChunk");

                let result = handle_audio_chunk(chunk, sess.clone(), executor.clone()).await;

                match result {
                    Ok(chunk_result) => {
                        let latency = chunk_start.elapsed().as_secs_f64();
                        metrics.record_chunk_processed(&session_id, latency);

                        // Log ChunkResult details before sending
                        info!(
                            "Sending ChunkResult: sequence={}, data_outputs count={}",
                            chunk_result.sequence,
                            chunk_result.data_outputs.len()
                        );
                        for (node_id, data_buffer) in &chunk_result.data_outputs {
                            use crate::generated::data_buffer::DataType;
                            match &data_buffer.data_type {
                                Some(DataType::Audio(audio)) => {
                                    info!(
                                        "ChunkResult output[{}]: Audio with {} bytes",
                                        node_id,
                                        audio.samples.len()
                                    );
                                }
                                Some(DataType::Text(text)) => {
                                    info!(
                                        "ChunkResult output[{}]: Text with {} bytes",
                                        node_id,
                                        text.text_data.len()
                                    );
                                }
                                _ => {
                                    info!("ChunkResult output[{}]: Other type", node_id);
                                }
                            }
                        }

                        // Send ChunkResult response
                        let response = StreamResponse {
                            response: Some(StreamResponseType::Result(chunk_result.clone())),
                        };
                        tx.send(Ok(response)).await.map_err(|_| {
                            ServiceError::Internal("Failed to send ChunkResult".to_string())
                        })?;

                        // Send periodic metrics
                        let sess_lock = sess.lock().await;
                        if sess_lock.chunks_processed % METRICS_UPDATE_INTERVAL == 0 {
                            let cached_nodes_count = global_node_cache.read().await.len() as u64;
                            let stream_metrics = sess_lock.create_metrics(cached_nodes_count);
                            drop(sess_lock);

                            let metrics_response = StreamResponse {
                                response: Some(StreamResponseType::Metrics(stream_metrics)),
                            };
                            tx.send(Ok(metrics_response)).await.map_err(|_| {
                                ServiceError::Internal("Failed to send StreamMetrics".to_string())
                            })?;
                        }
                    }
                    Err(e) => {
                        metrics.record_chunk_error(&session_id);
                        return Err(e);
                    }
                }
            }

            Some(StreamRequestType::DataChunk(data_chunk)) => {
                // Handle DataChunk (generic streaming)
                let sess = session.as_ref().ok_or_else(|| {
                    ServiceError::Validation("StreamInit required before DataChunk".to_string())
                })?;

                let chunk_start = Instant::now();
                debug!(sequence = data_chunk.sequence, "Processing DataChunk");

                // Feed the chunk to the session router
                let mut sess_guard = sess.lock().await;
                if let Some(router_input) = &sess_guard.router_input {
                    // Convert DataBuffer to RuntimeData
                    use crate::adapters::data_buffer_to_runtime_data;

                    let runtime_data = if let Some(buffer) = data_chunk.buffer {
                        data_buffer_to_runtime_data(&buffer).ok_or_else(|| {
                            ServiceError::Validation("Data conversion failed".to_string())
                        })?
                    } else if !data_chunk.named_buffers.is_empty() {
                        // For multi-input, just use the first buffer for now
                        let (_, buffer) =
                            data_chunk.named_buffers.into_iter().next().ok_or_else(|| {
                                ServiceError::Validation("No input data provided".to_string())
                            })?;
                        data_buffer_to_runtime_data(&buffer).ok_or_else(|| {
                            ServiceError::Validation("Data conversion failed".to_string())
                        })?
                    } else {
                        return Err(ServiceError::Validation(
                            "DataChunk must have buffer or named_buffers".to_string(),
                        ));
                    };

                    // Create DataPacket - the data should be sent TO the node specified in data_chunk.node_id
                    // not FROM it. We use "client" as the source since this is input from the client.
                    let packet = DataPacket {
                        data: runtime_data,
                        from_node: "client".to_string(), // Data comes from client
                        to_node: Some(data_chunk.node_id.clone()), // Send TO this node for processing
                        session_id: session_id.clone(),
                        sequence: data_chunk.sequence,
                        sub_sequence: 0,
                    };

                    // Send to router
                    router_input.send(packet).map_err(|e| {
                        ServiceError::Internal(format!("Failed to send to router: {}", e))
                    })?;

                    sess_guard.chunks_processed += 1;
                    drop(sess_guard);

                    let latency = chunk_start.elapsed().as_secs_f64();
                    metrics.record_chunk_processed(&session_id, latency);

                    debug!("Fed DataChunk to session router");

                    // Send periodic metrics
                    let sess_lock = sess.lock().await;
                    if sess_lock.chunks_processed % METRICS_UPDATE_INTERVAL == 0 {
                        let cached_nodes_count = global_node_cache.read().await.len() as u64;
                        let stream_metrics = sess_lock.create_metrics(cached_nodes_count);
                        drop(sess_lock);

                        let metrics_response = StreamResponse {
                            response: Some(StreamResponseType::Metrics(stream_metrics)),
                        };
                        tx.send(Ok(metrics_response)).await.map_err(|_| {
                            ServiceError::Internal("Failed to send StreamMetrics".to_string())
                        })?;
                    }
                } else {
                    return Err(ServiceError::Internal(
                        "Session router not initialized".to_string(),
                    ));
                }
            }

            Some(StreamRequestType::Control(control)) => {
                // Handle StreamControl (CLOSE or CANCEL)
                let sess = session.as_ref().ok_or_else(|| {
                    ServiceError::Validation("StreamInit required before StreamControl".to_string())
                })?;

                debug!(command = control.command, "Processing StreamControl");
                let closed = handle_stream_control(control, sess.clone()).await?;

                // Send StreamClosed response
                let response = StreamResponse {
                    response: Some(StreamResponseType::Closed(closed)),
                };
                tx.send(Ok(response)).await.map_err(|_| {
                    ServiceError::Internal("Failed to send StreamClosed".to_string())
                })?;

                // Cleanup session and metrics
                if let Some(session_arc) = sessions.write().await.remove(&session_id) {
                    // Shutdown router and all node processing
                    let mut sess_guard = session_arc.lock().await;
                    sess_guard.shutdown_router().await;
                    sess_guard.clear_node_cache();
                }
                metrics.record_stream_end();
                info!(session_id = %session_id, "Session closed");
                break; // Exit stream loop
            }

            None => {
                warn!("Received empty StreamRequest");
            }
        }
    }

    // If we exit loop without explicit close, cleanup
    if !session_id.is_empty() {
        if let Some(session_arc) = sessions.write().await.remove(&session_id) {
            // Shutdown router and all node processing
            let mut sess_guard = session_arc.lock().await;
            sess_guard.shutdown_router().await;
            sess_guard.clear_node_cache();
        }
        metrics.record_stream_end();
        info!(session_id = %session_id, "Session disconnected");
    }

    Ok(())
}

/// Handle StreamInit message
async fn handle_stream_init(
    init: StreamInit,
    sessions: &Arc<RwLock<HashMap<String, Arc<Mutex<StreamSession>>>>>,
    _executor: Arc<remotemedia_runtime_core::executor::Executor>,
) -> Result<(String, StreamReady), ServiceError> {
    // Validate client version (basic check)
    if init.client_version.is_empty() {
        return Err(ServiceError::Validation(
            "client_version required".to_string(),
        ));
    }

    // Deserialize manifest
    let manifest_proto = init
        .manifest
        .ok_or_else(|| ServiceError::Validation("manifest required in StreamInit".to_string()))?;

    let manifest = deserialize_manifest_from_proto(&manifest_proto)?;

    // Generate unique session ID
    let session_id = Uuid::new_v4().to_string();

    // Determine recommended chunk size (use client's suggestion or default)
    let recommended_chunk_size = if init.expected_chunk_size > 0 {
        init.expected_chunk_size
    } else {
        4096 // Default: 4096 samples (~256ms at 16kHz)
    };

    // Create session
    let session = Arc::new(Mutex::new(StreamSession::new(
        session_id.clone(),
        manifest,
        recommended_chunk_size,
    )));

    // Store session
    sessions.write().await.insert(session_id.clone(), session);

    info!(
        session_id = %session_id,
        chunk_size = recommended_chunk_size,
        "StreamSession created"
    );

    // Return StreamReady
    let ready = StreamReady {
        session_id: session_id.clone(),
        recommended_chunk_size,
        max_buffer_latency_ms: 100, // 100ms max buffer latency
    };

    Ok((session_id, ready))
}

/// Handle AudioChunk message
async fn handle_audio_chunk(
    chunk: AudioChunk,
    session: Arc<Mutex<StreamSession>>,
    executor: Arc<remotemedia_runtime_core::executor::Executor>,
) -> Result<ChunkResult, ServiceError> {
    let start_time = Instant::now();

    // Lock session to get manifest and validate
    let manifest = {
        let mut sess = session.lock().await;

        // Validate sequence number
        sess.validate_sequence(chunk.sequence)?;

        // Clone manifest for execution
        sess.manifest.clone()
    };

    // Deserialize audio buffer
    let buffer_proto = chunk
        .buffer
        .ok_or_else(|| ServiceError::Validation("AudioChunk.buffer required".to_string()))?;

    let audio_buffer = convert_proto_to_runtime_audio(&buffer_proto)?;
    let samples = buffer_proto.num_samples;

    // Build audio inputs map for the chunk
    // The chunk's node_id tells us which node should receive this audio
    let mut audio_inputs = HashMap::new();
    audio_inputs.insert(chunk.node_id.clone(), audio_buffer);

    // Execute pipeline with fast audio path
    let result_buffers = executor
        .execute_fast_pipeline(&manifest, audio_inputs)
        .await
        .map_err(|e| ServiceError::Internal(format!("Pipeline execution failed: {}", e)))?;

    let processing_time_ms = start_time.elapsed().as_secs_f64() * 1000.0;

    // Lock session again to record metrics and get total items
    let total_items = {
        let mut sess = session.lock().await;
        sess.record_chunk_metrics(processing_time_ms, samples, 0, "audio"); // TODO: Track memory
        sess.total_items
    };

    // Convert result buffers to proto format
    // Convert audio buffers to DataBuffer format
    let mut data_outputs = HashMap::new();
    for (node_id, buffer) in result_buffers {
        let proto_buffer = convert_runtime_to_proto_audio(&buffer);
        // Wrap audio buffer in DataBuffer
        let data_buffer = crate::generated::DataBuffer {
            data_type: Some(crate::generated::data_buffer::DataType::Audio(proto_buffer)),
            metadata: HashMap::new(),
        };
        data_outputs.insert(node_id, data_buffer);
    }

    let result = ChunkResult {
        sequence: chunk.sequence,
        data_outputs,
        processing_time_ms,
        total_items_processed: total_items,
    };

    Ok(result)
}

/// Recursively route output data through the pipeline
async fn route_to_downstream(
    output_data: RuntimeData,
    from_node_id: String,
    session: Arc<Mutex<StreamSession>>,
    streaming_registry: Arc<StreamingNodeRegistry>,
    tx: tokio::sync::mpsc::Sender<Result<StreamResponse, Status>>,
    session_id: String,
    base_sequence: u64,
) -> Result<(), ServiceError> {
    // USE THE NEW ASYNC ROUTER FOR TRUE STREAMING
    use crate::async_router::route_to_downstream_async;

    return route_to_downstream_async(
        output_data,
        from_node_id,
        session,
        streaming_registry,
        tx,
        session_id,
        base_sequence,
    )
    .await
    .map_err(|e| ServiceError::Internal(e.to_string()));
}

/// Handle DataChunk message with multi-output support (for streaming generators)
async fn handle_data_chunk_multi(
    chunk: crate::generated::DataChunk,
    session: Arc<Mutex<StreamSession>>,
    streaming_registry: Arc<StreamingNodeRegistry>,
    metrics: Arc<ServiceMetrics>,
    tx: tokio::sync::mpsc::Sender<Result<StreamResponse, Status>>,
    global_node_cache: Arc<RwLock<HashMap<String, CachedNode>>>,
) -> Result<usize, ServiceError> {
    let start_time = Instant::now();

    // Extract session_id for passing to Python nodes
    let session_id = {
        let sess = session.lock().await;
        sess.session_id.clone()
    };

    // Get or create node from cache (global cache with TTL)
    let (node, _py_streaming_node): (
        Arc<Box<dyn StreamingNode>>,
        Option<Arc<PythonStreamingNode>>,
    ) = {
        let mut sess = session.lock().await;
        sess.validate_sequence(chunk.sequence)?;

        // Get node info from manifest
        let node_spec = sess
            .manifest
            .nodes
            .iter()
            .find(|n| n.id == chunk.node_id)
            .ok_or_else(|| {
                ServiceError::Validation(format!("Node '{}' not found in manifest", chunk.node_id))
            })?;

        let node_type = node_spec.node_type.clone();
        let params = node_spec.params.clone();

        // Create cache key: "{node_type}:{params_hash}"
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        let mut hasher = DefaultHasher::new();
        params.hash(&mut hasher);
        let cache_key = format!("{}:{:x}", node_type, hasher.finish());

        // Check global cache first (read lock)
        let cache_entry = {
            let global_cache = global_node_cache.read().await;
            global_cache.get(&cache_key).map(|cached| {
                // info!("‚ôªÔ∏è Reusing globally cached node: {} (key: {})", chunk.node_id, cache_key);
                (Arc::clone(&cached.node), cached.py_streaming_node.clone())
            })
        };

        let (node, py_streaming_node) = if let Some((cached_node, cached_py_node)) = cache_entry {
            // CACHE HIT!
            sess.cache_hits += 1;
            metrics.record_cache_hit(&node_type);
            // info!("‚úÖ Cache HIT for node type '{}' (session hits: {}, misses: {})", node_type, sess.cache_hits, sess.cache_misses);

            // Update timestamp in global cache (write lock)
            let mut global_cache = global_node_cache.write().await;
            if let Some(cached) = global_cache.get_mut(&cache_key) {
                cached.last_used = Instant::now();
            }

            // Also store in session cache for quick lookup
            sess.node_cache
                .insert(chunk.node_id.clone(), Arc::clone(&cached_node));
            (cached_node, cached_py_node)
        } else {
            // CACHE MISS!
            sess.cache_misses += 1;
            metrics.record_cache_miss(&node_type);
            // info!("‚ùå Cache MISS for node type '{}' (session hits: {}, misses: {})", node_type, sess.cache_hits, sess.cache_misses);

            // Node not cached - create new instance
            // info!("üÜï Creating new node: {} (type: {}, key: {})", chunk.node_id, node_type, cache_key);

            // Check if this is a Python node via the registry
            // Python nodes need special handling to preserve the unwrapped instance for caching
            let (new_node, py_streaming_node) = if streaming_registry.is_python_node(&node_type) {
                use remotemedia_runtime_core::nodes::{
                    python_streaming::PythonStreamingNode, AsyncNodeWrapper,
                };

                info!(
                    "üêç Creating Python streaming node: {} with session {}",
                    node_type, session_id
                );

                let py_node = PythonStreamingNode::with_session(
                    chunk.node_id.clone(),
                    &node_type,
                    &params,
                    session_id.clone(),
                )
                .map_err(|e| {
                    ServiceError::Internal(format!("Failed to create Python streaming node: {}", e))
                })?;

                // Initialize the node immediately to load the model into memory
                // info!("üîß Initializing Python streaming node '{}'...", chunk.node_id);
                py_node.ensure_initialized().await.map_err(|e| {
                    ServiceError::Internal(format!("Failed to initialize Python node: {}", e))
                })?;
                // info!("‚úÖ Python streaming node '{}' initialized successfully", chunk.node_id);

                let py_node_arc = Arc::new(py_node);
                let wrapped: Box<dyn StreamingNode> =
                    Box::new(AsyncNodeWrapper(Arc::clone(&py_node_arc)));

                (wrapped, Some(py_node_arc))
            } else {
                // Regular Rust nodes - use registry normally
                // info!("ü¶Ä Creating Rust streaming node: {}", node_type);
                let node = streaming_registry
                    .create_node(
                        &node_type,
                        chunk.node_id.clone(),
                        &params,
                        Some(session_id.clone()),
                    )
                    .map_err(|e| ServiceError::Internal(format!("Failed to create node: {}", e)))?;
                (node, None)
            };

            // Wrap in Arc
            let arc_node = Arc::new(new_node);

            // Store in global cache with timestamp
            let mut global_cache = global_node_cache.write().await;
            global_cache.insert(
                cache_key.clone(),
                CachedNode {
                    node: Arc::clone(&arc_node),
                    py_streaming_node: py_streaming_node.clone(),
                    last_used: Instant::now(),
                },
            );

            // Update Prometheus gauge for cached nodes count
            metrics.set_cached_nodes_count(global_cache.len() as i64);

            // Also store in session cache for quick lookup
            sess.node_cache
                .insert(chunk.node_id.clone(), Arc::clone(&arc_node));

            // info!("üíæ Globally cached node '{}' (type: {}, key: {}, total cached: {})", chunk.node_id, node_type, cache_key, global_cache.len());
            (arc_node, py_streaming_node)
        };

        (node, py_streaming_node)
    };

    // Convert DataBuffer(s) to RuntimeData
    use crate::adapters::data_buffer_to_runtime_data;

    let (runtime_data_map, data_type, item_count) = if !chunk.named_buffers.is_empty() {
        // Multi-input mode
        let mut map = HashMap::new();
        let mut total_items = 0u64;
        let mut types = Vec::new();

        for (name, data_buffer) in chunk.named_buffers {
            let runtime_data = data_buffer_to_runtime_data(&data_buffer).ok_or_else(|| {
                ServiceError::Validation(format!("Data conversion failed for '{}'", name))
            })?;

            types.push(runtime_data.data_type().to_string());
            total_items += runtime_data.item_count() as u64;
            map.insert(name, runtime_data);
        }

        let combined_type = if types.len() == 1 {
            types[0].to_string()
        } else {
            format!("multi[{}]", types.join("+"))
        };

        (map, combined_type, total_items)
    } else if let Some(data_buffer) = chunk.buffer {
        // Single-input mode
        let runtime_data = data_buffer_to_runtime_data(&data_buffer)
            .ok_or_else(|| ServiceError::Validation("Data conversion failed".to_string()))?;

        let data_type = runtime_data.data_type().to_string();
        let item_count = runtime_data.item_count() as u64;

        let mut map = HashMap::new();
        map.insert("input".to_string(), runtime_data);

        (map, data_type, item_count)
    } else {
        return Err(ServiceError::Validation(
            "DataChunk must have either 'buffer' or 'named_buffers' set".to_string(),
        ));
    };

    // Extract input data for single-input nodes
    let input_data = runtime_data_map
        .get("input")
        .or_else(|| runtime_data_map.values().next())
        .ok_or_else(|| ServiceError::Validation("No input data provided".to_string()))?
        .clone();

    // Check if this is a streaming node (Python or Rust)
    let node_type = node.node_type();
    let output_count: usize;

    // Check if this is a multi-output streaming node
    let is_streaming = streaming_registry.is_multi_output_streaming(&node_type);

    // Use streaming path for multi-output streaming nodes (both Python and Rust)
    if is_streaming {
        // Multi-yield streaming node - use callback for incremental sending
        info!(
            "üéôÔ∏è Detected multi-yield streaming node '{}', using streaming iteration",
            node_type
        );

        // USE THE CACHED NODE instead of creating a new one!
        // The 'node' variable already contains our globally cached instance
        // which preserves the Python object and the loaded Kokoro model

        // Create a channel for chunks from Python -> Rust async world
        let (chunk_tx, mut chunk_rx) = tokio::sync::mpsc::unbounded_channel::<RuntimeData>();

        // Spawn async task to send chunks as they arrive from the channel
        let tx_clone = tx.clone();
        let session_clone = session.clone();
        let streaming_registry_clone = streaming_registry.clone();
        let chunk_node_id = chunk.node_id.clone();
        let base_sequence = chunk.sequence;
        let _data_type_clone = data_type.clone();
        let session_id_clone = session_id.clone();

        let send_task = tokio::spawn(async move {
            info!("üì° Send task started - waiting for chunks...");
            let mut chunk_idx = 0u64;

            while let Some(output_data) = chunk_rx.recv().await {
                info!(
                    "üéØ Received chunk {} from streaming node '{}' - immediately routing to client",
                    chunk_idx + 1,
                    chunk_node_id
                );

                // Use recursive routing
                if let Err(e) = route_to_downstream(
                    output_data,
                    chunk_node_id.clone(),
                    session_clone.clone(),
                    streaming_registry_clone.clone(),
                    tx_clone.clone(),
                    session_id_clone.clone(),
                    base_sequence + chunk_idx,
                )
                .await
                {
                    error!("Routing failed: {}", e);
                }

                chunk_idx += 1;
            }

            chunk_idx
        });

        // Process streaming with callback that just enqueues chunks
        // Use the unified trait method for both Python and Rust nodes

        // Spawn the streaming processing as a separate task so it doesn't block
        let chunk_tx_clone = chunk_tx.clone();
        let node_clone = Arc::clone(&node); // Clone the Arc
        let session_id_clone = session_id.clone();

        // Start the process_task IMMEDIATELY without blocking
        let process_task = tokio::spawn(async move {
            info!("üöÄ Process task started");
            let result = node_clone
                .process_streaming_async(
                    input_data,
                    Some(session_id_clone),
                    Box::new(move |output_data| {
                        info!("üì® Callback called - sending chunk to channel");
                        // Unbounded channels don't have try_send, just use send which never blocks
                        if let Err(e) = chunk_tx_clone.send(output_data) {
                            error!("Failed to send chunk to channel: {:?}", e);
                            return Err(remotemedia_runtime_core::Error::Execution(
                                "Failed to enqueue chunk".to_string(),
                            ));
                        }
                        info!("üì® Chunk sent to channel successfully");
                        Ok(())
                    }),
                )
                .await;
            info!("üèÅ Process task completed");
            result
        });

        // Drop our reference to chunk_tx so it closes when process_task completes
        drop(chunk_tx);

        // CRITICAL: Don't wait for process_task to complete before starting to send!
        // The tasks should run truly concurrently

        // Wait for both tasks to complete
        let (send_result, process_result) = tokio::join!(send_task, process_task);

        // Check process task result
        process_result
            .map_err(|e| ServiceError::Internal(format!("Process task panicked: {}", e)))?
            .map_err(|e| ServiceError::Internal(format!("Multi-chunk streaming failed: {}", e)))?;

        // Get output count from send task
        output_count = send_result
            .map_err(|e| ServiceError::Internal(format!("Send task failed: {}", e)))?
            as usize;

        debug!("‚úÖ Completed streaming {} chunks", output_count);
    } else {
        // Regular node - single output
        use crate::adapters::runtime_data_to_data_buffer;

        let output = node
            .process_async(input_data)
            .await
            .map_err(|e| ServiceError::Internal(format!("Node execution failed: {}", e)))?;

        let output_buffer = runtime_data_to_data_buffer(&output);
        let mut data_outputs = HashMap::new();
        data_outputs.insert(chunk.node_id.clone(), output_buffer);

        let total_items = {
            let mut sess = session.lock().await;
            sess.record_chunk_metrics(0.0, item_count, 0, &data_type);
            sess.total_items
        };

        let chunk_result = ChunkResult {
            sequence: chunk.sequence,
            data_outputs,
            processing_time_ms: 0.0,
            total_items_processed: total_items,
        };

        let response = StreamResponse {
            response: Some(StreamResponseType::Result(chunk_result)),
        };
        tx.send(Ok(response))
            .await
            .map_err(|_| ServiceError::Internal("Failed to send ChunkResult".to_string()))?;

        output_count = 1;
    }

    let processing_time_ms = start_time.elapsed().as_secs_f64() * 1000.0;
    debug!(
        "Total processing time: {:.2}ms for {} chunks",
        processing_time_ms, output_count
    );

    Ok(output_count)
}

/// Handle StreamControl message
async fn handle_stream_control(
    control: StreamControl,
    session: Arc<Mutex<StreamSession>>,
) -> Result<StreamClosed, ServiceError> {
    let sess = session.lock().await;

    let command = Command::try_from(control.command)
        .map_err(|_| ServiceError::Validation(format!("Invalid command: {}", control.command)))?;

    let reason = match command {
        Command::Close => "Client requested close",
        Command::Cancel => "Client requested cancel",
        Command::Unspecified => "Unspecified",
    };

    let closed = StreamClosed {
        session_id: sess.session_id.clone(),
        final_metrics: Some(sess.create_final_metrics()),
        reason: reason.to_string(),
    };

    info!(
        session_id = %sess.session_id,
        chunks_processed = sess.chunks_processed,
        reason = reason,
        "Stream closing"
    );

    Ok(closed)
}

/// Helper: Deserialize protobuf PipelineManifest to runtime Manifest
fn deserialize_manifest_from_proto(
    proto: &crate::generated::PipelineManifest,
) -> Result<Manifest, ServiceError> {
    // Convert to JSON for existing Manifest parser
    let json_str = serde_json::json!({
        "version": proto.version,
        "metadata": serde_json::json!({
            "name": proto.metadata.as_ref().map(|m| m.name.clone()).unwrap_or_default(),
            "description": proto.metadata.as_ref().map(|m| m.description.clone()).unwrap_or_default(),
            "created_at": proto.metadata.as_ref().map(|m| m.created_at.clone()).unwrap_or_else(|| "2025-10-28T00:00:00Z".to_string()),
        }),
        "nodes": proto.nodes.iter().map(|n| {
            serde_json::json!({
                "id": n.id,
                "node_type": n.node_type,
                "params": serde_json::from_str::<serde_json::Value>(&n.params)
                    .unwrap_or(serde_json::json!({})),
                "runtime_hint": match n.runtime_hint {
                    0 => "auto",
                    1 => "rust_python",
                    2 => "cpython",
                    3 => "cpython_wasm",
                    _ => "auto",
                },
                "metadata": serde_json::json!({
                    "name": n.node_type,
                    "description": "",
                    "created_at": "2025-10-28T00:00:00Z",
                })
            })
        }).collect::<Vec<_>>(),
        "connections": proto.connections.iter().map(|c| {
            serde_json::json!({
                "from": c.from,
                "to": c.to
            })
        }).collect::<Vec<_>>()
    })
    .to_string();

    serde_json::from_str(&json_str)
        .map_err(|e| ServiceError::Validation(format!("Failed to parse manifest: {}", e)))
}

/// Helper: Convert protobuf AudioBuffer to runtime AudioBuffer
fn convert_proto_to_runtime_audio(
    proto: &ProtoAudioBuffer,
) -> Result<RuntimeAudioBuffer, ServiceError> {
    use crate::generated::AudioFormat as ProtoAudioFormat;
    use remotemedia_runtime_core::audio::AudioFormat;

    // Convert format and decode bytes to f32 samples
    let samples: Vec<f32> = match ProtoAudioFormat::try_from(proto.format) {
        Ok(ProtoAudioFormat::F32) => {
            // Convert bytes to f32 (little-endian)
            proto
                .samples
                .chunks_exact(4)
                .map(|chunk| f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
                .collect()
        }
        Ok(ProtoAudioFormat::I16) => {
            // Convert bytes to i16, then to f32 normalized (-1.0 to 1.0)
            proto
                .samples
                .chunks_exact(2)
                .map(|chunk| {
                    let sample = i16::from_le_bytes([chunk[0], chunk[1]]);
                    sample as f32 / 32768.0
                })
                .collect()
        }
        Ok(ProtoAudioFormat::I32) => {
            // Convert bytes to i32, then to f32 normalized
            proto
                .samples
                .chunks_exact(4)
                .map(|chunk| {
                    let sample = i32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]);
                    sample as f32 / 2147483648.0
                })
                .collect()
        }
        _ => {
            return Err(ServiceError::Validation(format!(
                "Unsupported audio format: {}",
                proto.format
            )));
        }
    };

    // Create runtime AudioBuffer
    Ok(RuntimeAudioBuffer::new(
        Arc::new(samples),
        proto.sample_rate,
        proto.channels as u16,
        AudioFormat::F32,
    ))
}

/// Helper: Convert runtime AudioBuffer to protobuf AudioBuffer
fn convert_runtime_to_proto_audio(buffer: &RuntimeAudioBuffer) -> ProtoAudioBuffer {
    use crate::generated::AudioFormat as ProtoAudioFormat;
    use remotemedia_runtime_core::audio::AudioFormat;

    let format = match buffer.format() {
        AudioFormat::F32 => ProtoAudioFormat::F32 as i32,
        AudioFormat::I16 => ProtoAudioFormat::I16 as i32,
        AudioFormat::I32 => ProtoAudioFormat::I32 as i32,
    };

    // Convert f32 samples to bytes based on format
    let samples: Vec<u8> = match buffer.format() {
        AudioFormat::F32 => {
            // Convert f32 to bytes (little-endian)
            buffer
                .as_slice()
                .iter()
                .flat_map(|&sample| sample.to_le_bytes())
                .collect()
        }
        AudioFormat::I16 => {
            // Convert f32 to i16, then to bytes
            buffer
                .as_slice()
                .iter()
                .flat_map(|&sample| {
                    let i_sample = (sample * 32768.0).clamp(-32768.0, 32767.0) as i16;
                    i_sample.to_le_bytes()
                })
                .collect()
        }
        AudioFormat::I32 => {
            // Convert f32 to i32, then to bytes
            buffer
                .as_slice()
                .iter()
                .flat_map(|&sample| {
                    let i_sample =
                        (sample * 2147483648.0).clamp(-2147483648.0, 2147483647.0) as i32;
                    i_sample.to_le_bytes()
                })
                .collect()
        }
    };

    ProtoAudioBuffer {
        samples,
        sample_rate: buffer.sample_rate(),
        channels: buffer.channels() as u32,
        format,
        num_samples: buffer.len_samples() as u64,
    }
}
